[
  {
    "text": "from above 80% to less than 40% (Yang et al., 2019a).",
    "element_type": "TEXT",
    "bbox": [
      72.0,
      755.9363424,
      267.80824320000016,
      764.9027424
    ],
    "page_number": 1,
    "metadata": "ElementMetadata(style_info={'font_name': 'ZJXLJS+NimbusRomNo9L-Regu', 'font_size': 8.966400000000007, 'is_bold': False, 'is_italic': False, 'is_monospace': False, 'char_count': 43, 'avg_char_width': 4.059485469767445, 'line_count': 1, 'text_density': 0.03018747506780983, 'median_font_size': 10.909099999999995}, coordinates=CoordinatesMetadata(x0=72.0, y0=755.9363424, x1=267.80824320000016, y1=764.9027424, page_width=595.276, page_height=841.89, relative_x=0.1209522977576788, relative_y=0.8979039332929479, indent_ratio=0.1209522977576788), list_group_id=None, confidence=0.7, cross_page_refs=[], page_number=None)"
  },
  {
    "text": "2For instance, the exact match score on SQuAD v1.1 drops",
    "element_type": "LIST_ITEM",
    "bbox": [
      84.653,
      744.5075616,
      290.26801721600015,
      754.9397424
    ],
    "page_number": 1,
    "metadata": "ElementMetadata(style_info={'font_name': 'ZJXLJS+NimbusRomNo9L-Regu', 'font_size': 5.977599999999995, 'is_bold': False, 'is_italic': False, 'is_monospace': False, 'char_count': 47, 'avg_char_width': 3.9889822066383, 'line_count': 1, 'text_density': 0.02610706745911188, 'median_font_size': 10.909099999999995}, coordinates=CoordinatesMetadata(x0=84.653, y0=744.5075616, x1=290.26801721600015, y1=754.9397424, page_width=595.276, page_height=841.89, relative_x=0.14220798419556643, relative_y=0.8843287859459075, indent_ratio=0.14220798419556643), list_group_id='3da739e7-20fa-4575-a624-7f9d8ddcbaca', confidence=0.9, cross_page_refs=[], page_number=None)"
  },
  {
    "text": "https://github.com/facebookresearch/DPR.",
    "element_type": "TEXT",
    "bbox": [
      72.0,
      735.0993424,
      224.46466559999996,
      744.0657424
    ],
    "page_number": 1,
    "metadata": "ElementMetadata(style_info={'font_name': 'ZJXLJS+NimbusRomNo9L-Regu', 'font_size': 8.966400000000007, 'is_bold': False, 'is_italic': False, 'is_monospace': False, 'char_count': 40, 'avg_char_width': 3.822824640000004, 'line_count': 1, 'text_density': 0.029259889150647433, 'median_font_size': 10.909099999999995}, coordinates=CoordinatesMetadata(x0=72.0, y0=735.0993424, x1=224.46466559999996, y1=744.0657424, page_width=595.276, page_height=841.89, relative_x=0.1209522977576788, relative_y=0.8731536690066398, indent_ratio=0.1209522977576788), list_group_id=None, confidence=0.7, cross_page_refs=[], page_number=None)"
  },
  {
    "text": "∗Equal contribution\n1The code and trained models have been released at",
    "element_type": "TEXT",
    "bbox": [
      84.653,
      712.6650544,
      290.2687345280001,
      734.1027424
    ],
    "page_number": 1,
    "metadata": "ElementMetadata(style_info={'font_name': 'ZHJZJA+CMSY6', 'font_size': 5.977599999999981, 'is_bold': False, 'is_italic': False, 'is_monospace': False, 'char_count': 60, 'avg_char_width': 3.9862831082666674, 'line_count': 2, 'text_density': 0.01588048450916552, 'median_font_size': 10.909099999999995}, coordinates=CoordinatesMetadata(x0=84.653, y0=712.6650544, x1=290.2687345280001, y1=734.1027424, page_width=595.276, page_height=841.89, relative_x=0.14220798419556643, relative_y=0.8465061402320969, indent_ratio=0.14220798419556643), list_group_id=None, confidence=0.7, cross_page_refs=[], page_number=None)"
  },
  {
    "text": "However, it is generally believed that learn-\ning a good dense vector representation needs a\nlarge number of labeled pairs of question and con-\ntexts. Dense retrieval methods have thus never\nbe shown to outperform TF-IDF/BM25 for open-\ndomain QA before ORQA (Lee et al., 2019), which\nproposes a sophisticated inverse cloze task (ICT)\nobjective, predicting the blocks that contain the\nmasked sentence, for additional pretraining. The\nquestion encoder and the reader model are then ﬁne-\ntuned using pairs of questions and answers jointly.\nAlthough ORQA successfully demonstrates that\ndense retrieval can outperform BM25, setting new\nstate-of-the-art results on multiple open-domain",
    "element_type": "TEXT",
    "bbox": [
      306.883,
      578.2732656000001,
      527.4561434833,
      765.3223656
    ],
    "page_number": 1,
    "metadata": "ElementMetadata(style_info={'font_name': 'ZJXLJS+NimbusRomNo9L-Regu', 'font_size': 10.909099999999995, 'is_bold': False, 'is_italic': False, 'is_monospace': False, 'char_count': 579, 'avg_char_width': 4.7817362504386915, 'line_count': 14, 'text_density': 0.016457411139406516, 'median_font_size': 10.909099999999995}, coordinates=CoordinatesMetadata(x0=306.883, y0=578.2732656000001, x1=527.4561434833, y1=765.3223656, page_width=595.276, page_height=841.89, relative_x=0.5155306110106909, relative_y=0.686875085343691, indent_ratio=0.5155306110106909), list_group_id=None, confidence=0.7, cross_page_refs=[], page_number=None)"
  },
  {
    "text": "Open-domain question answering (QA) (Voorhees,\n1999) is a task that answers factoid questions us-\ning a large collection of documents. While early\nQA systems are often complicated and consist of\nmultiple components (Ferrucci (2012); Moldovan\net al. (2003), inter alia), the advances of reading\ncomprehension models suggest a much simpliﬁed\ntwo-stage framework: (1) a context retriever ﬁrst\nselects a small subset of passages where some\nof them contain the answer to the question, and\nthen (2) a machine reader can thoroughly exam-\nine the retrieved contexts and identify the correct\nanswer (Chen et al., 2017). Although reducing\nopen-domain QA to machine reading is a very rea-\nsonable strategy, a huge performance degradation\nis often observed in practice2, indicating the needs\nof improving retrieval.",
    "element_type": "TEXT",
    "bbox": [
      71.182,
      478.7092656,
      292.0834997664,
      706.4053656
    ],
    "page_number": 1,
    "metadata": "ElementMetadata(style_info={'font_name': 'ZJXLJS+NimbusRomNo9L-Regu', 'font_size': 10.909100000000024, 'is_bold': False, 'is_italic': False, 'is_monospace': False, 'char_count': 681, 'avg_char_width': 4.790619065396475, 'line_count': 17, 'text_density': 0.015964719367947522, 'median_font_size': 10.909099999999995}, coordinates=CoordinatesMetadata(x0=71.182, y0=478.7092656, x1=292.0834997664, y1=706.4053656, page_width=595.276, page_height=841.89, relative_x=0.11957814526370962, relative_y=0.5686126044970246, indent_ratio=0.11957814526370962), list_group_id=None, confidence=0.7, cross_page_refs=[], page_number=None)"
  },
  {
    "text": "1 1 Introduction Introduction",
    "element_type": "LIST_ITEM",
    "bbox": [
      71.99999999999997,
      457.2434368,
      154.81367039999998,
      469.1986368
    ],
    "page_number": 1,
    "metadata": "ElementMetadata(style_info={'font_name': 'YGCXDP+NimbusRomNo9L-Medi', 'font_size': 11.95519999999999, 'is_bold': False, 'is_italic': False, 'is_monospace': False, 'char_count': 1, 'avg_char_width': 5.977599999999995, 'line_count': 1, 'text_density': 0.013993176237682804, 'median_font_size': 10.909099999999995}, coordinates=CoordinatesMetadata(x0=71.99999999999997, y0=457.2434368, x1=154.81367039999998, y1=469.1986368, page_width=595.276, page_height=841.89, relative_x=0.12095229775767875, relative_y=0.5431154150779793, indent_ratio=0.12095229775767875), list_group_id='08eb3195-a772-48ac-a062-10d98389b825', confidence=0.9, cross_page_refs=[], page_number=None)"
  },
  {
    "text": "Open-domain question answering relies on ef-\nﬁcient passage retrieval to select candidate\ncontexts, where traditional sparse vector space\nmodels, such as TF-IDF or BM25, are the de\nfacto method.\nIn this work, we show that\nretrieval can be practically implemented us-\ning dense representations alone, where em-\nbeddings are learned from a small number\nof questions and passages by a simple dual-\nencoder framework. When evaluated on a\nwide range of open-domain QA datasets, our\ndense retriever outperforms a strong Lucene-\nBM25 system greatly by 9%-19% absolute in\nterms of top-20 passage retrieval accuracy, and\nhelps our end-to-end QA system establish new\nstate-of-the-art on multiple open-domain QA\nbenchmarks.1",
    "element_type": "TEXT",
    "bbox": [
      88.64899999999997,
      245.65132159999985,
      274.9155202,
      446.8969216
    ],
    "page_number": 1,
    "metadata": "ElementMetadata(style_info={'font_name': 'ZJXLJS+NimbusRomNo9L-Regu', 'font_size': 9.962600000000066, 'is_bold': False, 'is_italic': False, 'is_monospace': False, 'char_count': 606, 'avg_char_width': 4.3868648501650185, 'line_count': 18, 'text_density': 0.019020780543312594, 'median_font_size': 10.909099999999995}, coordinates=CoordinatesMetadata(x0=88.64899999999997, y0=245.65132159999985, x1=274.9155202, y1=446.8969216, page_width=595.276, page_height=841.89, relative_x=0.14892083672111756, relative_y=0.29178553207663693, indent_ratio=0.14892083672111756), list_group_id=None, confidence=0.7, cross_page_refs=[], page_number=None)"
  },
  {
    "text": "Retrieval in open-domain QA is usually imple-\nmented using TF-IDF or BM25 (Robertson and\nZaragoza, 2009), which matches keywords efﬁ-\nciently with an inverted index and can be seen\nas representing the question and context in high-\ndimensional, sparse vectors (with weighting). Con-\nversely, the dense, latent semantic encoding is com-\nplementary to sparse representations by design. For\nexample, synonyms or paraphrases that consist of\ncompletely different tokens may still be mapped to\nvectors close to each other. Consider the question\n“Who is the bad guy in lord of the rings?”, which can\nbe answered from the context “Sala Baker is best\nknown for portraying the villain Sauron in the Lord\nof the Rings trilogy.” A term-based system would\nhave difﬁculty retrieving such a context, while\na dense retrieval system would be able to better\nmatch “bad guy” with “villain” and fetch the cor-\nrect context. Dense encodings are also learnable\nby adjusting the embedding functions, which pro-\nvides additional ﬂexibility to have a task-speciﬁc\nrepresentation. With special in-memory data struc-\ntures and indexing schemes, retrieval can be done\nefﬁciently using maximum inner product search\n(MIPS) algorithms (e.g., Shrivastava and Li (2014);\nGuo et al. (2016)). Retrieval in open-domain QA is usually imple-\nmented using TF-IDF or BM25 (Robertson and\nZaragoza, 2009), which matches keywords efﬁ-\nciently with an inverted index and can be seen\nas representing the question and context in high-\ndimensional, sparse vectors (with weighting). Con-\nversely, the dense, latent semantic encoding is com-\nplementary to sparse representations by design. For\nexample, synonyms or paraphrases that consist of\ncompletely different tokens may still be mapped to\nvectors close to each other. Consider the question\n“Who is the bad guy in lord of the rings?”, which can\nbe answered from the context “Sala Baker is best\nknown for portraying the villain Sauron in the Lord\nof the Rings trilogy.” A term-based system would\nhave difﬁculty retrieving such a context, while\na dense retrieval system would be able to better\nmatch “bad guy” with “villain” and fetch the cor-\nrect context. Dense encodings are also learnable\nby adjusting the embedding functions, which pro-\nvides additional ﬂexibility to have a task-speciﬁc\nrepresentation. With special in-memory data struc-\ntures and indexing schemes, retrieval can be done\nefﬁciently using maximum inner product search\n(MIPS) algorithms (e.g., Shrivastava and Li (2014);\nGuo et al. (2016)). Abstract Abstract",
    "element_type": "TEXT",
    "bbox": [
      158.89099999999996,
      224.04343679999988,
      527.3584524928,
      574.5863655999999
    ],
    "page_number": 1,
    "metadata": "ElementMetadata(style_info={'font_name': 'ZJXLJS+NimbusRomNo9L-Regu', 'font_size': 10.909099999999967, 'is_bold': False, 'is_italic': False, 'is_monospace': False, 'char_count': 1061, 'avg_char_width': 4.756163432234971, 'line_count': 26, 'text_density': 0.016013606678818862, 'median_font_size': 10.909099999999995}, coordinates=CoordinatesMetadata(x0=158.89099999999996, y0=224.04343679999988, x1=527.3584524928, y1=574.5863655999999, page_width=595.276, page_height=841.89, relative_x=0.2669198825418797, relative_y=0.2661196080248012, indent_ratio=0.2669198825418797), list_group_id=None, confidence=0.7, cross_page_refs=[], page_number=None)"
  },
  {
    "text": "{vladk, barlaso, plewis, ledell, edunov, scottyih}@fb.com\nsewon@cs.washington.edu\ndanqic@cs.princeton.edu",
    "element_type": "TEXT",
    "bbox": [
      97.02799999999996,
      147.3852144,
      503.50383999999997,
      187.23641439999994
    ],
    "page_number": 1,
    "metadata": "ElementMetadata(style_info={'font_name': 'TIWTGF+CMSY10', 'font_size': 11.95519999999999, 'is_bold': False, 'is_italic': False, 'is_monospace': False, 'char_count': 98, 'avg_char_width': 7.148721632653053, 'line_count': 3, 'text_density': 0.006482061668010938, 'median_font_size': 10.909099999999995}, coordinates=CoordinatesMetadata(x0=97.02799999999996, y0=147.3852144, x1=503.50383999999997, y1=187.23641439999994, page_width=595.276, page_height=841.89, relative_x=0.1629966603726674, relative_y=0.17506469301215122, indent_ratio=0.1629966603726674), list_group_id=None, confidence=0.7, cross_page_refs=[], page_number=None)"
  },
  {
    "text": "Facebook AI",
    "element_type": "TITLE",
    "bbox": [
      127.37199999999999,
      134.26212320000002,
      189.2760256,
      146.2173232
    ],
    "page_number": 1,
    "metadata": "ElementMetadata(style_info={'font_name': 'ZJXLJS+NimbusRomNo9L-Regu', 'font_size': 11.95519999999999, 'is_bold': False, 'is_italic': False, 'is_monospace': False, 'char_count': 10, 'avg_char_width': 5.909455360000004, 'line_count': 1, 'text_density': 0.014863358305765805, 'median_font_size': 10.909099999999995}, coordinates=CoordinatesMetadata(x0=127.37199999999999, y0=134.26212320000002, x1=189.2760256, y1=146.2173232, page_width=595.276, page_height=841.89, relative_x=0.21397133430543142, relative_y=0.15947703761774107, indent_ratio=0.21397133430543142), list_group_id=None, confidence=0.7, cross_page_refs=[], page_number=None)"
  },
  {
    "text": "†University of Washington †University of Washington ‡Princeton University ‡Princeton University",
    "element_type": "TEXT",
    "bbox": [
      216.17399999999998,
      132.8730994,
      473.161728,
      146.2173232
    ],
    "page_number": 1,
    "metadata": "ElementMetadata(style_info={'font_name': 'ZLDFBT+CMSY8', 'font_size': 7.970100000000002, 'is_bold': False, 'is_italic': False, 'is_monospace': False, 'char_count': 23, 'avg_char_width': 5.303326035652172, 'line_count': 1, 'text_density': 0.014749640778354927, 'median_font_size': 10.909099999999995}, coordinates=CoordinatesMetadata(x0=216.17399999999998, y0=132.8730994, x1=473.161728, y1=146.2173232, page_width=595.276, page_height=841.89, relative_x=0.3631491946592841, relative_y=0.157827150102745, indent_ratio=0.3631491946592841), list_group_id=None, confidence=0.7, cross_page_refs=[], page_number=None)"
  },
  {
    "text": "Vladimir Karpukhin∗, Barlas O˘guz∗, Sewon Min†, Patrick Lewis,\nLedell Wu, Sergey Edunov, Danqi Chen‡, Wen-tau Yih",
    "element_type": "TEXT",
    "bbox": [
      138.733,
      104.75809939999999,
      461.8008112,
      132.07663679999996
    ],
    "page_number": 1,
    "metadata": "ElementMetadata(style_info={'font_name': 'YGCXDP+NimbusRomNo9L-Medi', 'font_size': 11.95519999999999, 'is_bold': False, 'is_italic': False, 'is_monospace': False, 'char_count': 98, 'avg_char_width': 5.8534116275510195, 'line_count': 2, 'text_density': 0.012803458852999328, 'median_font_size': 10.909099999999995}, coordinates=CoordinatesMetadata(x0=138.733, y0=104.75809939999999, x1=461.8008112, y1=132.07663679999996, page_width=595.276, page_height=841.89, relative_x=0.23305659895577852, relative_y=0.1244320509805319, indent_ratio=0.23305659895577852), list_group_id=None, confidence=0.7, cross_page_refs=[], page_number=None)"
  },
  {
    "text": "Dense Passage Retrieval for Open-Domain Question Answering",
    "element_type": "TEXT",
    "bbox": [
      104.748,
      70.34515579999993,
      492.79836379999995,
      84.6913558
    ],
    "page_number": 1,
    "metadata": "ElementMetadata(style_info={'font_name': 'YGCXDP+NimbusRomNo9L-Medi', 'font_size': 14.346200000000067, 'is_bold': False, 'is_italic': False, 'is_monospace': False, 'char_count': 52, 'avg_char_width': 7.065227611538459, 'line_count': 1, 'text_density': 0.010418447740073847, 'median_font_size': 10.909099999999995}, coordinates=CoordinatesMetadata(x0=104.748, y0=70.34515579999993, x1=492.79836379999995, y1=84.6913558, page_width=595.276, page_height=841.89, relative_x=0.17596543452112973, relative_y=0.08355623157419607, indent_ratio=0.17596543452112973), list_group_id=None, confidence=0.9, cross_page_refs=[], page_number=None)"
  },
  {
    "text": "4Exceptions include (Seo et al., 2019) and (Roberts et al.,\n2020), which retrieves and generates the answers, respectively.",
    "element_type": "LIST_ITEM",
    "bbox": [
      307.276,
      744.5075616,
      526.6681527168001,
      764.9027424
    ],
    "page_number": 2,
    "metadata": "ElementMetadata(style_info={'font_name': 'ZJXLJS+NimbusRomNo9L-Regu', 'font_size': 5.977599999999995, 'is_bold': False, 'is_italic': False, 'is_monospace': False, 'char_count': 106, 'avg_char_width': 3.713865398339622, 'line_count': 2, 'text_density': 0.027488843089642816, 'median_font_size': 10.909099999999995}, coordinates=CoordinatesMetadata(x0=307.276, y0=744.5075616, x1=526.6681527168001, y1=764.9027424, page_width=595.276, page_height=841.89, relative_x=0.5161908089692849, relative_y=0.8843287859459075, indent_ratio=0.5161908089692849), list_group_id='fe276995-22c1-4ad7-aabf-dadcc0045d54', confidence=0.9, cross_page_refs=[], page_number=None)"
  },
  {
    "text": "3The ideal size and boundary of a text passage are func-\ntions of both the retriever and reader. We also experimented\nwith natural paragraphs in our preliminary trials and found that\nusing ﬁxed-length passages performs better in both retrieval\nand ﬁnal QA accuracy, as observed by Wang et al. (2019).",
    "element_type": "LIST_ITEM",
    "bbox": [
      306.953,
      693.7825616,
      527.0354881920002,
      744.0657424
    ],
    "page_number": 2,
    "metadata": "ElementMetadata(style_info={'font_name': 'ZJXLJS+NimbusRomNo9L-Regu', 'font_size': 5.977599999999995, 'is_bold': False, 'is_italic': False, 'is_monospace': False, 'char_count': 251, 'avg_char_width': 3.877696912267722, 'line_count': 5, 'text_density': 0.02710897049744125, 'median_font_size': 10.909099999999995}, coordinates=CoordinatesMetadata(x0=306.953, y0=693.7825616, x1=527.0354881920002, y1=744.0657424, page_width=595.276, page_height=841.89, relative_x=0.5156482035223997, relative_y=0.824077446697312, indent_ratio=0.5156482035223997), list_group_id='cc540b11-a925-42ef-b34e-3d654833a7c5', confidence=0.9, cross_page_refs=[], page_number=None)"
  },
  {
    "text": "The problem of open-domain QA studied in this\npaper can be described as follows. Given a factoid\nquestion, such as “Who ﬁrst voiced Meg on Family\nGuy?” or “Where was the 8th Dalai Lama born?”, a\nsystem is required to answer it using a large corpus\nof diversiﬁed topics. More speciﬁcally, we assume",
    "element_type": "TEXT",
    "bbox": [
      71.607,
      686.6672656000001,
      290.27304174600005,
      765.3223656
    ],
    "page_number": 2,
    "metadata": "ElementMetadata(style_info={'font_name': 'ZJXLJS+NimbusRomNo9L-Regu', 'font_size': 10.909099999999995, 'is_bold': False, 'is_italic': False, 'is_monospace': False, 'char_count': 245, 'avg_char_width': 4.84161825360245, 'line_count': 6, 'text_density': 0.017268245657005403, 'median_font_size': 10.909099999999995}, coordinates=CoordinatesMetadata(x0=71.607, y0=686.6672656000001, x1=290.27304174600005, y1=765.3223656, page_width=595.276, page_height=841.89, relative_x=0.1202920997990848, relative_y=0.8156258722635975, indent_ratio=0.1202920997990848), list_group_id=None, confidence=0.7, cross_page_refs=[], page_number=None)"
  },
  {
    "text": "2 Background",
    "element_type": "TEXT",
    "bbox": [
      72.0,
      662.4454367999999,
      152.82910720000004,
      674.4006368
    ],
    "page_number": 2,
    "metadata": "ElementMetadata(style_info={'font_name': 'YGCXDP+NimbusRomNo9L-Medi', 'font_size': 11.95519999999999, 'is_bold': False, 'is_italic': False, 'is_monospace': False, 'char_count': 11, 'avg_char_width': 6.280827345454545, 'line_count': 1, 'text_density': 0.01241814190594526, 'median_font_size': 10.909099999999995}, coordinates=CoordinatesMetadata(x0=72.0, y0=662.4454367999999, x1=152.82910720000004, y1=674.4006368, page_width=595.276, page_height=841.89, relative_x=0.1209522977576788, relative_y=0.7868550960339236, indent_ratio=0.1209522977576788), list_group_id=None, confidence=0.7, cross_page_refs=[], page_number=None)"
  },
  {
    "text": "Our dense passage retriever (DPR) uses a dense\nencoder EP (·) which maps any text passage to a d-\ndimensional real-valued vectors and builds an index\nfor all the M passages that we will use for retrieval.",
    "element_type": "TEXT",
    "bbox": [
      307.276,
      633.0282655999999,
      527.4560805519999,
      684.5843656
    ],
    "page_number": 2,
    "metadata": "ElementMetadata(style_info={'font_name': 'ZJXLJS+NimbusRomNo9L-Regu', 'font_size': 10.909099999999995, 'is_bold': False, 'is_italic': False, 'is_monospace': False, 'char_count': 168, 'avg_char_width': 4.696056296357152, 'line_count': 4, 'text_density': 0.01797099336167645, 'median_font_size': 10.909099999999995}, coordinates=CoordinatesMetadata(x0=307.276, y0=633.0282655999999, x1=527.4560805519999, y1=684.5843656, page_width=595.276, page_height=841.89, relative_x=0.5161908089692849, relative_y=0.7519132732304695, indent_ratio=0.5161908089692849), list_group_id=None, confidence=0.7, cross_page_refs=[], page_number=None)"
  },
  {
    "text": "3.1 Overview",
    "element_type": "TITLE",
    "bbox": [
      307.276,
      615.0599019,
      376.43969400000003,
      625.9690019
    ],
    "page_number": 2,
    "metadata": "ElementMetadata(style_info={'font_name': 'YGCXDP+NimbusRomNo9L-Medi', 'font_size': 10.909099999999995, 'is_bold': False, 'is_italic': False, 'is_monospace': False, 'char_count': 11, 'avg_char_width': 5.315706909090915, 'line_count': 1, 'text_density': 0.015904284744162167, 'median_font_size': 10.909099999999995}, coordinates=CoordinatesMetadata(x0=307.276, y0=615.0599019, x1=376.43969400000003, y1=625.9690019, page_width=595.276, page_height=841.89, relative_x=0.5161908089692849, relative_y=0.730570385561059, indent_ratio=0.5161908089692849), list_group_id=None, confidence=1.0, cross_page_refs=[], page_number=None)"
  },
  {
    "text": "We focus our research in this work on improv-\ning the retrieval component in open-domain QA.\nGiven a collection of M text passages, the goal of\nour dense passage retriever (DPR) is to index all\nthe passages in a low-dimensional and continuous\nspace, such that it can retrieve efﬁciently the top\nk passages relevant to the input question for the\nreader at run-time. Note that M can be very large\n(e.g., 21 million passages in our experiments, de-\nscribed in Section 4.1) and k is usually small, such\nas 20–100.",
    "element_type": "TEXT",
    "bbox": [
      306.763,
      456.53426559999997,
      527.4521391259999,
      602.9353656
    ],
    "page_number": 2,
    "metadata": "ElementMetadata(style_info={'font_name': 'ZJXLJS+NimbusRomNo9L-Regu', 'font_size': 10.909100000000024, 'is_bold': False, 'is_italic': False, 'is_monospace': False, 'char_count': 420, 'avg_char_width': 4.735202699245721, 'line_count': 11, 'text_density': 0.015754059519797265, 'median_font_size': 10.909099999999995}, coordinates=CoordinatesMetadata(x0=306.763, y0=456.53426559999997, x1=527.4521391259999, y1=602.9353656, page_width=595.276, page_height=841.89, relative_x=0.5153290238477614, relative_y=0.5422730589506942, indent_ratio=0.5153290238477614), list_group_id=None, confidence=0.7, cross_page_refs=[], page_number=None)"
  },
  {
    "text": "3 Dense Passage Retriever (DPR)",
    "element_type": "TEXT",
    "bbox": [
      307.276,
      434.1994368,
      485.13351040000003,
      446.1546368
    ],
    "page_number": 2,
    "metadata": "ElementMetadata(style_info={'font_name': 'YGCXDP+NimbusRomNo9L-Medi', 'font_size': 11.95519999999999, 'is_bold': False, 'is_italic': False, 'is_monospace': False, 'char_count': 27, 'avg_char_width': 5.827938607407412, 'line_count': 1, 'text_density': 0.014579164595286902, 'median_font_size': 10.909099999999995}, coordinates=CoordinatesMetadata(x0=307.276, y0=434.1994368, x1=485.13351040000003, y1=446.1546368, page_width=595.276, page_height=841.89, relative_x=0.5161908089692849, relative_y=0.5157436681751773, indent_ratio=0.5161908089692849), list_group_id=None, confidence=0.7, cross_page_refs=[], page_number=None)"
  },
  {
    "text": "In this paper, we address the question: can we\ntrain a better dense embedding model using only\npairs of questions and passages (or answers), with-\nout additional pretraining? By leveraging the now\nstandard BERT pretrained model (Devlin et al.,\n2019) and a dual-encoder architecture (Bromley\net al., 1994), we focus on developing the right\ntraining scheme using a relatively small number\nof question and passage pairs. Through a series\nof careful ablation studies, our ﬁnal solution is\nsurprisingly simple: the embedding is optimized\nfor maximizing inner products of the question and\nrelevant passage vectors, with an objective compar-\ning all pairs of questions and passages in a batch.\nOur Dense Passage Retriever (DPR) is exception-\nally strong. It not only outperforms BM25 by a\nlarge margin (65.2% vs. 42.9% in Top-5 accuracy),\nbut also results in a substantial improvement on\nthe end-to-end QA accuracy compared to ORQA\n(41.5% vs. 33.3%) in the open Natural Questions\nsetting (Lee et al., 2019; Kwiatkowski et al., 2019).\nOur contributions are twofold. First, we demon-\nstrate that with the proper training setup, sim-\nply ﬁne-tuning the question and passage encoders\non existing question-passage pairs is sufﬁcient to\ngreatly outperform BM25. Our empirical results\nalso suggest that additional pretraining may not be\nneeded. Second, we verify that, in the context of\nopen-domain question answering, a higher retrieval\nprecision indeed translates to a higher end-to-end\nQA accuracy. By applying a modern reader model\nto the top retrieved passages, we achieve compara-\nble or better results on multiple QA datasets in the\nopen-retrieval setting, compared to several, much\ncomplicated systems.",
    "element_type": "TEXT",
    "bbox": [
      71.64,
      174.5782656,
      292.175528934,
      647.1313656
    ],
    "page_number": 2,
    "metadata": "ElementMetadata(style_info={'font_name': 'ZJXLJS+NimbusRomNo9L-Regu', 'font_size': 10.909099999999967, 'is_bold': False, 'is_italic': False, 'is_monospace': False, 'char_count': 1434, 'avg_char_width': 4.736694270558787, 'line_count': 35, 'text_density': 0.01627408821554244, 'median_font_size': 10.909099999999995}, coordinates=CoordinatesMetadata(x0=71.64, y0=174.5782656, x1=292.175528934, y1=647.1313656, page_width=595.276, page_height=841.89, relative_x=0.1203475362688904, relative_y=0.2073646980009265, indent_ratio=0.1203475362688904), list_group_id=None, confidence=0.7, cross_page_refs=[], page_number=None)"
  },
  {
    "text": "QA datasets, it also suffers from two weaknesses.\nFirst, ICT pretraining is computationally intensive\nand it is not completely clear that regular sentences\nare good surrogates of questions in the objective\nfunction. Second, because the context encoder is\nnot ﬁne-tuned using pairs of questions and answers,\nthe corresponding representations could be subop-\ntimal. the extractive QA setting, in which the answer is\nrestricted to a span appearing in one or more pas-\nsages in the corpus. Assume that our collection\ncontains D documents, d1, d2, · · · , dD. We ﬁrst\nsplit each of the documents into text passages of\nequal lengths as the basic retrieval units3 and get M\ntotal passages in our corpus C = {p1, p2, . . . , pM },\nwhere each passage pi can be viewed as a sequence\n2 , · · · , w(i)\n1 , w(i)\nof tokens w(i)\n|pi|. Given a question q,\nthe task is to ﬁnd a span w(i)\ns+1, · · · , w(i)\ns , w(i)\nfrom\none of the passages pi that can answer the question.\nNotice that to cover a wide variety of domains, the\ncorpus size can easily range from millions of docu-\nments (e.g., Wikipedia) to billions (e.g., the Web).\nAs a result, any open-domain QA system needs to\ninclude an efﬁcient retriever component that can se-\nlect a small set of relevant texts, before applying the\nreader to extract the answer (Chen et al., 2017).4\nFormally speaking, a retriever R : (q, C) → CF\nis a function that takes as input a question q and a\ncorpus C and returns a much smaller ﬁlter set of\ntexts CF ⊂ C, where |CF | = k (cid:28) |C|. For a ﬁxed\nk, a retriever can be evaluated in isolation on top-k\nretrieval accuracy, which is the fraction of ques-\ntions for which CF contains a span that answers the\nquestion.",
    "element_type": "TEXT",
    "bbox": [
      72.0,
      65.21426559999998,
      527.4571756539999,
      421.5223656
    ],
    "page_number": 2,
    "metadata": "ElementMetadata(style_info={'font_name': 'ZJXLJS+NimbusRomNo9L-Regu', 'font_size': 10.909099999999967, 'is_bold': False, 'is_italic': False, 'is_monospace': False, 'char_count': 1054, 'avg_char_width': 4.633726756774295, 'line_count': 32, 'text_density': 0.016884591685624003, 'median_font_size': 10.909099999999995}, coordinates=CoordinatesMetadata(x0=72.0, y0=65.21426559999998, x1=527.4571756539999, y1=421.5223656, page_width=595.276, page_height=841.89, relative_x=0.1209522977576788, relative_y=0.07746174155768566, indent_ratio=0.1209522977576788), list_group_id=None, confidence=0.7, cross_page_refs=[], page_number=None)"
  }
]